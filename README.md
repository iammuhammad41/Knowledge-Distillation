# Knowledge-Distillation
Knowledge distillation involves transferring knowledge from a large, complex model (teacher) to a smaller, simpler model (student). This technique is useful for deploying deep learning models on resource-constrained devices while maintaining performance.
