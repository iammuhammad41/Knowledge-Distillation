# Knowledge-Distillation
Knowledge distillation involves transferring knowledge from a large, complex model (teacher) to a smaller, simpler model (student). This technique is useful for deploying deep learning models on resource-constrained devices while maintaining performance.


References:

https://arxiv.org/abs/1503.02531

Keras: 
https://keras.io/examples/vision/knowledge_distillation/

Pytorch:
https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html

https://github.com/yoshitomo-matsubara/torchdistill

https://www.kaggle.com/code/shivangitomar/knowledge-distillation-part-1-pytorch

https://github.com/haitongli/knowledge-distillation-pytorch
